\documentclass{article}
\usepackage[left=.75in, right=.75in, top=0.5in, bottom=.75in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{float}
\title{COMP 576 - Fall 2017\\ Assignment 1}
\author{Alberto Fung - NetID: af31}

\begin{document}

\maketitle
\pagenumbering{arabic}

\section*{Backpropagation in a Simple Neural Network}

\subsection*{1a) Dataset}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/makemoondata.png}
		\caption{Make Moon Dataset}
	\end{figure}


\subsection*{1b) Derivatives of Activation Functions}
Sigmoid: 
	\begin{align*}
		f(x) &= \frac{1}{1+e^{-x}} = (1+e^{-x})^{-1}\\ \\
		\frac{d(1+e^{-x})^{-1}}{dx} &= (1+e^{-x})^{-2} (e^{-x})\\\\
		&=\frac{e^{-x}}{(1+e^{-x})^{2}} = \frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}}\\\\
		&=\frac{1}{1+e^{-x}}\frac{(1+e^{-x}) - 1}{1+e^{-x}}\\\\
		&= \frac{1}{1+e^{-x}}\left (1 - \frac{ 1}{1+e^{-x}}\right)\\\\
		\boldsymbol{\frac{d(1+e^{-x})^{-1}}{dx}} &\boldsymbol{= f(x)(1-f(x))}\\
	\end{align*}
Tanh:
	\begin{align*}
		f(x) &= \tanh{x} = \frac{\sinh{x}}{\cosh{x}}\\ \\
		\frac{d\left(\frac{\sinh{x}}{\cosh{x}}\right)}{dx}&= \frac{\cosh{x}\cosh{x} - \sinh{x}\sinh{x}}{\cosh^{2}{x}}\\\\
		&= \frac{\cosh^{2}{x}-\sinh^{2}{x}}{\cosh^{2}{x}} = \frac{\cosh^{2}{x}}{\cosh^{2}{x}} -  \frac{\sinh^{2}{x}}{\cosh^{2}{x}}\\\\
		\boldsymbol{\frac{d\left(\frac{\sinh{x}}{\cosh{x}}\right)}{dx}}& \boldsymbol{=1-\tanh^{2}{x}}\\
	\end{align*}
ReLu:
	\begin{equation*}
		f(x) = max(0,x)
	\end{equation*}
\\
	\[
		f(x) = 
			\begin{cases}
				x, & x> 0\\
				0, & \text {otherwise}
			\end{cases}
	\]
\\
	\[
		f'(x) = 
			\begin{cases}
				1, & x> 0\\
				0, & \text {otherwise}
			\end{cases}
	\]

\subsection*{1c) Building the Neural Network}
Three Layer Network
\begin{align}
	z^{1} &= W^{1}x + b^{1}\\
	a^{1} &=actFun(z^{1})\\
	z^{2} &=W^{2}a^{1}+b^{2}\\
	a^{2} &=\hat{y}=softmax(z^{2})
\end{align}
Mean Cross Entropy Loss of Batch
\begin{equation}
	L(y,\hat{y}) = -\frac{1}{N}\sum_{n=1}^{N}\sum_{i=1}^{C}y_{n,i}\log{\hat{y}_{n,i}}
\end{equation}

\subsection*{1d) Backward Pass - Backpropagation}
Gradients: $\frac{\partial L}{\partial W^{2}}$, $\frac{\partial L}{\partial b^{2}}$, $\frac{\partial L}{\partial W^{1}}$, $\frac{\partial L}{\partial b^{1}}$\\ \\
	\begin{align*}
		\frac{\partial L}{\partial W^{2}} &= \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial z^{2}}\frac{\partial z^{2}}{\partial W^{2}}\\ \\
		\frac{\partial z^{2}}{\partial W^{2}} &= \frac{\partial (W^{2}a^{2}+b^{2})}{\partial W^{2}}\\ \\
		\boldsymbol{\frac{\partial z^{2}}{\partial W^{2}}}& \boldsymbol{=a^{2}}
	\\
	\\
		\frac{\partial \hat{y_i}}{\partial z_i^{2}} &= \frac{\partial \left(\frac{e^{z_i^2}}{\sum_{j=1}^Ce^{z_j^2}}\right)}{\partial z_i^2}\\ \\
		&  \textbf{if }   j = i\\ \\		
		& = \frac{e^{z_i^2}\sum_{j=1}^Ce^{z_j^2}-e^{z_i^2}e^{z_i^2}}{\left(\sum_{j=1}^Ce^{z_j^2}\right)^2} = \frac{e^{z_i^2}(\sum_{j=1}^Ce^{z_j^2}-e^{z_i^2})}{\sum_{j=1}^Ce^{z_j^2}\sum_{j=1}^Ce^{z_j^2}}\\ \\
		\boldsymbol{\frac{\partial \hat{y_i}}{\partial z_i^{2}}} & \boldsymbol{=\hat{y_i}(1-\hat{y_i})}
	\end{align*}
	\begin{align*}
		&  \textbf{if }   j \neq i\\ \\
		\frac{\partial \hat{y_i}}{\partial z_j^{2}}& = \frac{0-e^{z_i^2}e^{z_j^2}}{(\sum_{j=1}^Ce^{z_j^2})^2} \\\\
		\boldsymbol{\frac{\partial \hat{y_i}}{\partial z_j^{2}}} & \boldsymbol{=-\hat{y_i}\hat{y_j}}
	\\
	\\
		\frac{\partial L}{\partial \hat{y_i}} &= -\frac{1}{N}\sum_{n=1}^N\sum_{i=1}^C\frac{\partial (y_i\log{\hat{y_i}})}{\partial \hat{y_i}} = -\frac{1}{N}\sum_{n=1}^N\sum_{i=1}^C y_i \left(\frac{1}{\hat{y_i}} \right) \frac{\partial \hat{y_i}}			  			{\partial z_i^2}\\ \\
		&  \forall i \\ \\
		&= -\frac{1}{N}\sum_{n=1}^N \left[  \frac{ y_i}{\hat{y_i}} \hat{y_i} \left(1-\hat{y_i} \right) + \sum_{j\neq i}^C \frac{y_j}{\hat{y_j}}  (-\hat{y_j}\hat{y_i}) \right]\\ \\
		&= -\frac{1}{N}\sum_{n=1}^N \left[ y_i(1-\hat{y_i}) - \sum_{j\neq i}^C y_j\hat{y_i} \right]\\ \\
		&= \frac{1}{N}\sum_{n=1}^N \left[ -y_i+y_i\hat{y_i} +  \sum_{j\neq i}^Cy_j\hat{y_i} \right]\\
		&= \frac{1}{N}\sum_{n=1}^N \left[ -y_i+ \sum_{j= 1}^Cy_j\hat{y_i} \right]\\ \\
		&= \text{since } \boldsymbol{y_j} \text{ is one-hot encoded}\\ \\
		&\implies \sum_{j=1}^Cy_j = 1\\ \\
		\boldsymbol{\therefore{} \frac{\partial L}{\partial z_i^2}} &= \boldsymbol{\frac{1}{N}\sum_{n=1}^N(\hat{y_i}-y_i)}
	\\
	\\
		& \textbf{Let } \delta^3 = (\hat{y_i}-y_i)\\ \\
		\frac{\partial L}{\partial W^2} &= \frac{1}{N}\sum_{n=1}^N\delta^3a^3\\\\
		\boldsymbol{\frac{\partial L}{\partial W^2}}& \boldsymbol{=\frac{1}{N}a^{1T}\delta^3} \\ \\
	\end{align*}
	\begin{align*}
		\frac{\partial L}{\partial b^2} &= \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial z^2}\frac{\partial z^2}{\partial b^2}\\ \\
		\frac{\partial z^2}{\partial b^2} &= 1\\\\
		\boldsymbol{\frac{\partial L}{\partial b^2}} & \boldsymbol{=\frac{1}{N}\sum_{n=1}^C\delta^3}
	\\
	\\
		\frac{\partial L}{\partial W^1} &=\underbrace{ \frac{\partial L}{\partial \hat{y}}\frac{\partial \hat{y}}{\partial z^2}}_{\frac{1}{N}\sum_{i=1}^N\delta^3}
			\overbrace{\frac{\partial z^2}{\partial a^1}}^{W^2}
			\underbrace{\frac{\partial a^1}{\partial z^1}}_{f'(z^1)}
			\overbrace{\frac{\partial z^1}{\partial W^1}}^x\\ \\
		&= \frac{1}{N}W^{2T}\delta^3f'(z^1)\\ \\
		& \textbf{Let } \delta^2 = W^{2T}\delta^3f'(z^1)\\ \\
		&= \frac{1}{N}\delta^2x\\ \\
		\boldsymbol{\frac{\partial L}{\partial W^1}} & \boldsymbol{= \frac{1}{N}x^T\delta^2}
	\\
	\\
		\frac{\partial L}{\partial b^1} &= \underbrace{\frac{\partial L}{\partial \hat{y}}
			\frac{\partial \hat{y}}{\partial z^2}
			\frac{\partial z^2}{\partial a^1}
			\frac{\partial a^1}{\partial z^1}}_{\frac{1}{N}\delta^2}
			\overbrace{\frac{\partial z^1}{\partial b^1}}^1\\ \\
		\boldsymbol{\frac{\partial L}{\partial b^1}} &\boldsymbol{= \frac{1}{N}\delta^2}\\ \\
	\end{align*}
	
\subsection*{1e) Training Network}
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/sigmoid.png}
		\caption{Using Sigmoid Activation Function with 3 Hidden Neurons and stepsize = 0.01}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/tanh.png}
		\caption{Using Tanh Activation Function with 3 Hidden Neurons and stepsize = 0.01}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/relu.png}
		\caption{Using ReLu Activation Function with 3 Hidden Neurons and stepsize = 0.01}
	\end{figure}
	
\paragraph*{\\ \\Training Using more Neurons}.

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/sigmoid_nnh50.png}
		\caption{Using Sigmoid Activation Function with 50 Hidden Neurons and stepsize = 0.01}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/Tanh_nnh50.png}
		\caption{Using Tanh Activation Function with 50 Hidden Neurons and stepsize = 0.01}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/ReLu_nnh50.png}
		\caption{Using ReLu Activation Function with 50 Hidden Neurons and stepsize = 0.01}
	\end{figure}

As you increase the number of neurons in the hidden layer, the decision boundary improves in both cases where the activation function is Tanh and ReLu, where as the Sigmoid case remains constant. In both Tanh and ReLu accuracy increased when the number of hidden was increased. 

\subsection*{1f) Training a Deep Network}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/Deep_sigmoid_10_20.png}
		\caption{Using Sigmoid Activation Function with 10 Hidden Layers and 20 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}
The sigmoid function with this architecture performed so poorly that it was not able to draw a decision boundary seperating the dataset

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/Deep_tanh_10_20.png}
		\caption{Using Tanh Activation Function with 10 Hidden Layers and 20 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/Deep_Relu_10_20.png}
		\caption{Using ReLu Activation Function with 10 Hidden Layers and 20 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/Deep_sigmoid_2_10.png}
		\caption{Using Sigmoid Activation Function with 2 Hidden Layers and 10 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/Deep_tanh_2_10.png}
		\caption{Using Tanh Activation Function with 2 Hidden Layers and 10 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}
Using a deep Network the ReLu activation function works the best. In both the Sigmoid and Tanh activation cases, having a deeper network produces worse results.

Training with a new dataset. I decided to use the make circle dataset to see how well the network performs where seperation is highly non-linear.

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/sigmoid2.png}
		\caption{Using Sigmoid Activation Function with 2 Hidden Layers and 10 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/relu2.png}
		\caption{Using ReLu Activation Function with 2 Hidden Layers and 10 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}


	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/relu3.png}
		\caption{Using ReLu Activation Function with 6 Hidden Layers and 10 Hidden Neurons/layer and stepsize = 0.01}
	\end{figure}

Once again, the ReLu activation functions performs the best. Also as you increase the depth of the network the performance starts the decrease\\\\

\section*{Training a Simple Deep Convolutional Network on MNIST}
\subsection*{2.a) Final Test Accuracy}
The final Test accuracy for this run is 0.9865
	\begin{figure}[H]
		\centering
		\includegraphics[width=12cm]{figures/tensorboard1.png}
		\caption{Training Loss}
	\end{figure}

\subsection*{2.b) More on Visualizing Your Training}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/1a.png}
	\end{figure}

	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/2a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/3a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/4a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/5a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/6a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/7a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/8a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/9a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/10a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/11a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/12a.png}
	\end{figure}
\subsection*{2.c) Time for More Fun!!!}
Training with Tanh activation function and momentum optimizer. A final accuracy of 0.9125
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/13.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/13a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/14a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/15a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/16a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/17a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/18a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/19a.png}
	\end{figure}
	\begin{figure}[H]
		\centering
		\includegraphics[width=13cm]{figures/20a.png}
	\end{figure}
\end{document}