\documentclass{article}
\usepackage[left=.75in, right=.75in, top=0.5in, bottom=.75in]{geometry}

\usepackage{amsmath}
\title{COMP 576 - Fall 2017\\ Assignment 1}

\begin{document}

\maketitle
\pagenumbering{arabic}

\section*{Backpropagation in a Simple Neural Network}

\subsection*{1a) Dataset}

\subsection*{1b) Derivatives of Activation Functions}
Sigmoid: 
	\begin{align*}
		f(x) &= \frac{1}{1+e^{-x}} = (1+e^{-x})^{-1}\\ \\
		\frac{d(1+e^{-x})^{-1}}{dx} &= (1+e^{-x})^{-2} (e^{-x})\\\\
		&=\frac{e^{-x}}{(1+e^{-x})^{2}} = \frac{1}{1+e^{-x}}\frac{e^{-x}}{1+e^{-x}}\\\\
		&=\frac{1}{1+e^{-x}}\frac{(1+e^{-x}) - 1}{1+e^{-x}}\\\\
		&= \frac{1}{1+e^{-x}}\left (1 - \frac{ 1}{1+e^{-x}}\right)\\\\
		\frac{d(1+e^{-x})^{-1}}{dx} &= f(x)(1-f(x))\\
	\end{align*}
Tanh:
	\begin{align*}
		f(x) &= tanh(x) = \frac{sinh(x)}{cosh(x)}\\ \\
		\frac{d\left(\frac{sinh(x)}{cosh(x)}\right)}{dx}&= \frac{cosh(x)cosh(x) - sinh(x)sinh(x)}{cosh^{2}(x)}\\\\
		&= \frac{cosh^{2}(x)-sinh^{2}(x)}{cosh^{2}(x)} = \frac{cosh^{2}(x)}{cosh^{2}(x)} -  \frac{sinh^{2}(x)}{cosh^{2}(x)}\\\\
		\frac{d\left(\frac{sinh(x)}{cosh(x)}\right)}{dx}&=1-tanh^{2}(x)\\
	\end{align*}
ReLu:
	\begin{equation*}
		f(x) = max(0,x)
	\end{equation*}
\\
	\[
		f(x) = 
			\begin{cases}
				x, & x> 0\\
				0, & \text {otherwise}
			\end{cases}
	\]
\\
	\[
		f'(x) = 
			\begin{cases}
				1, & x> 0\\
				0, & \text {otherwise}
			\end{cases}
	\]

\subsection*{1c) Building the Neural Network}
Three Layer Network
\begin{align}
	z^{1} &= W^{1}x + b^{1}\\
	a^{1} &=actFun(z^{1})\\
	z^{2} &=W^{2}a^{1}+b^{2}\\
	a^{2} &=\hat{y}=softmax(z^{2})
\end{align}
Cross Entropy of Batch
\begin{equation}
	L(y,\hat{y}) = -\frac{1}{N}\sum_{n=1}^{N}\sum_{i=1}^{C}y_{n,i}log{\hat{y}_{ni}}
\end{equation}

\subsection*{1d) Backward Pass - Backpropagation}
Gradients

\end{document}